[#deployansible]
= Deploying with Ansible
include::_attributes.adoc[]

So far, we've manually deployed and configured Skupper and the application using `skupper` and `kubectl`.
One way to automate it is using https://www.ansible.com/[Ansible].

[#ansible]
== Ansible

Ansible is an open-source IT automation engine that automates provisioning, configuration management, application deployment, orchestration, and many other IT processes.
There are different ways to https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html[install Ansible].
The easiest way is using `pip`/`pipx`.

[.console-input]
[source, bash]
----
pipx install --include-deps ansible

python3 -m pip install --user ansible
----

With Ansible installed, we need to install Ansible and Kubernetes Ansible modules.

=== Installing required Ansible modules

To install Skupper and Kubernetes modules, you must use the `ansible-galaxy` command already installed with Ansible.

In a terminal window, run the following command:

[.console-input]
[source, bash]
----
ansible-galaxy collection install kubernetes.core

ansible-galaxy collection install skupper.network
----

With modules installed, we can move forward to store the Kubernetes `kubeconfig` files of both clusters in the local directory:

=== Kubeconfigs

To store the `kubeconfig` of the *cluster 1* export `KUBECONFIG` environment variable to the following value and then run `oc login` to create the file with the required values:

[.console-input]
[source, bash]
----
export KUBECONFIG=`pwd`/publick8sconfig

oc login --token=sha256~sq_--server=https://api.sandbox-m4.g2pi.p1.openshiftapps.com:6443 // <1>
----
<1> Change this to your specific arguments.

Now, we need to store the *cluster 2* information into another `kubeconfig` file.
First, unset the previous `KUBECONFIG`, switch to _kind_ context, and finally use the `kind` tool to materialize the file:

[.console-input]
[source, bash]
----
unset KUBECONFIG

kubectx kind-kind-cluster
kind export kubeconfig --name kind-cluster --kubeconfig `pwd`/privatek8sconfig
----

We can now refer to these files in Ansible to log in to the clusters.
Let's deploy the same application seen on xref:02-onprem.adoc[Chapter 2] but using Ansible instead of using CLI commands.

[#ansiblemanifests]
==  Manifests

=== Application manifests

First, we must create the Kubernetes manifests to deploy the application.
They are the same ones used before, but we'll paste them here again as a reminder.
Store them in the `kubernetes` folder so that you can refer them from the Ansible playbook:

[.console-input]
[source, yaml]
.kubernetes/public.yaml
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-connection
data:
  QUARKUS_DATASOURCE_JDBC_URL: "jdbc:postgresql://postgres:5432/psdb"
---
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: passport-storage
    app.kubernetes.io/version: 1.0.0-SNAPSHOT
    app.kubernetes.io/managed-by: quarkus
  name: passport-storage
spec:
  ports:
 - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app.kubernetes.io/name: passport-storage
    app.kubernetes.io/version: 1.0.0-SNAPSHOT
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: passport-storage
    app.kubernetes.io/version: 1.0.0-SNAPSHOT
    app.kubernetes.io/managed-by: quarkus
  name: passport-storage
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: passport-storage
      app.kubernetes.io/version: 1.0.0-SNAPSHOT
  template:
    metadata:
      labels:
        app.kubernetes.io/managed-by: quarkus
        app.kubernetes.io/name: passport-storage
        app.kubernetes.io/version: 1.0.0-SNAPSHOT
    spec:
      containers:
 - env:
 - name: KUBERNETES_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          envFrom:
 - configMapRef:
                name: postgres-connection
          image: quay.io/rhdevelopers/passport-storage:1.0.0-SNAPSHOT
          imagePullPolicy: Always
          name: passport-storage
          ports:
 - containerPort: 8080
              name: http
              protocol: TCP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: passport-storage
spec:
  tls:
    termination: edge
  to:
    kind: Service
    name: passport-storage
    weight: 100
  wildcardPolicy: None
----

And the database manifest:

[.console-input]
[source, yaml]
.kubernetes/private.yaml
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-secret
  labels:
    app: postgres
data:
  POSTGRES_DB: psdb
  POSTGRES_USER: psuser
  POSTGRES_PASSWORD: pspassword
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
 - name: postgres
          image: 'postgres:14'
          imagePullPolicy: Always
          ports:
 - containerPort: 5432
          envFrom:
 - configMapRef:
                name: postgres-secret
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  annotations:
    skupper.io/proxy: "tcp" # <1>
  labels:
    app: postgres
spec:
  type: "ClusterIP"
  ports:
 - port: 5432
  selector:
    app: postgres
----

=== Ansible Manifests

We need to create the two most important files in Ansible, the inventory and the playbook file.
In this case, we will call *cluster 1* west and *cluster 2* east, as these names make the playbook look clearer.

=== Inventory

Create an `inventory.yaml` file configuring clusters and its relationship with the following content:

[.console-input]
[source, yaml]
.inventory.yaml
----
all:
  vars:
    ansible_connection: local
  hosts:
    west: # <1>
      kubeconfig: "{{ inventory_dir }}/publick8sconfig" # <2>
      namespace: asotobue-dev # <3>
    east: # <4>
      kubeconfig: "{{ inventory_dir }}/privatek8sconfig"
      namespace: default
      links: # <5>
 - host: west
      services: # <6>
        postgres:
          ports:
 - 5432
          targets:
 - type: deployment
              name: postgres
----
<1> Defines the west cluster.
<2> Sets the `kubeconfig` file created in the previous section
<3> Sets the namespace
<4> Defines the east cluster
<5> Defines the link between clusters
<6> Exposes the Postgres service to the west

=== Playbook

The last thing before we can run Ansible is to create the playbook file, which does two things: installs the app to both clusters and installs and configures Skupper:
Create the following file:

[.console-input]
[source, yaml]
.playbook.yaml
----
- hosts: localhost
  tasks: # <1>
 - name: Create To public
      kubernetes.core.k8s: # <2>
        src: ./kubernetes/public.yaml
        namespace: asotobue-dev
        kubeconfig: ./publick8sconfig

 - name: Create To private
      kubernetes.core.k8s:
        src: ./kubernetes/private.yaml
        namespace: default
        kubeconfig: ./privatek8sconfig

- hosts: all
  collections:
 - skupper.network # <3>
  tasks:
 - import_role:
        name: skupper # <4>
----
<1> Apply the application manifests to both clusters
<2> Set the deployment file location and the cluster
<3> Import Skupper collection
<4> Apply for the role of installing Skupper

[#installappansible]
== Install the Application

To install the application and the Skupper proxy in both clusters, run the following command:

[.console-input]
[source, bash]
----
ansible-playbook -i inventory.yaml playbook.yaml
----

[.console-output]
[source, bash, subs="attributes+,+macros"]
----
PLAY [localhost] 

TASK [Gathering Facts] 
ok: [localhost]

...

TASK [skupper.network.skupper_common : Defining default facts] 
ok: [west]
ok: [east]

...

PLAY RECAP 
east                       : ok=8    changed=1    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0
localhost                  : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
west                       : ok=8    changed=1    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0
----

[#cleanupansible]
== Clean Up

To clean up the clusters, create a new playbook invoking `skupper_delete` role.

[.console-input]
[source, yaml]
.playbook-delete.yaml
----
- hosts: all
  collections:
 - skupper.network
  tasks:
 - import_role:
        name: skupper_delete

- hosts: localhost
  tasks:
 - name: Delete public
      kubernetes.core.k8s:
        state: absent
        src: ./kubernetes/public.yaml
        namespace: asotobue-dev
        kubeconfig: ./publick8sconfig

 - name: Delete private
      kubernetes.core.k8s:
        state: absent
        src: ./kubernetes/private.yaml
        namespace: default
        kubeconfig: ./privatek8sconfig
----

[.console-input]
[source, bash]
----
ansible-playbook -i inventory.yaml playbook-delete.yaml
----